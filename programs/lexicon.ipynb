{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"tf-small.png\"/>\n",
    "\n",
    "# Lexicon\n",
    "\n",
    "This notebook can read lexicon info in files issued by the etcbc and transform them \n",
    "into new features.\n",
    "There will be features at the word level and a new level will be made: lexeme.\n",
    "\n",
    "Most lexical features do not go to the word nodes but to the lexeme nodes.\n",
    "\n",
    "**NB** This conversion will not work for versions `4` and `4b`.\n",
    "\n",
    "## Discussion\n",
    "There are several issues that we deal with here.\n",
    "\n",
    "Language: are an Aramaic lexeme and a Hebrew lexeme with the same value identical?\n",
    "In short: no.\n",
    "\n",
    "The lexicon is a piece of data not conceptually contained in the text.\n",
    "So where do we leave that data?\n",
    "\n",
    "The lexicon contains information about lexemes. Some of that information is also present\n",
    "on individual occurrences. \n",
    "The question arises, should a lexical feature have consistent values across its occurrences.\n",
    "\n",
    "And of course: does the lexicon *match* the text?\n",
    "Do all lexemes in the text have a lexical entry, and does every lexical entry have actual\n",
    "occurrences in the text?\n",
    "\n",
    "### Lexeme language\n",
    "Lexemes do not cross languages, so the set of Aramaic and Hebrew lexemes are disjoint.\n",
    "Whenever we index lexemes, we have to specify it as a pair of its language and lex values.\n",
    "\n",
    "### Lexeme node type\n",
    "The answer where to leave the lexical information in a text-fabric data set is surprisingly simple:\n",
    "on nodes of a new type `lex`.\n",
    "Nodes of type lex will be connected via the `oslots` feature to all its occurrences, so lexemes *contain* there\n",
    "occurrences.\n",
    "All features encountered in the lexicon, we will store on these `lex` nodes.\n",
    "\n",
    "### Lexical consistency\n",
    "It is quite possible that some occurrences have got a different value for a lexical feature than its lexeme.\n",
    "A trivial case are adjectives, whose lexical gender is `NA`, but whose occurrences usually have a distinct gender.\n",
    "\n",
    "Other features really should be fully consistent, for example the *vocalized lexeme*.\n",
    "We encounter this feature in the text (`g_voc_lex` and also its hebrew version `g_voc_lex_utf8`), \n",
    "and in the lexicon it is present as feature `vc`.\n",
    "In this case we observe a deficiency in the lexicon: `vc` is often absent.\n",
    "Apart from that, the textual features `g_voc_lex` and `g_voc_lex_utf8` are fully consistent, so I take their values\n",
    "and put them in the lexicon, and I remove the `g_voc_lex` and `g_voc_lex_utf8` from the dataset.\n",
    "\n",
    "## Match between lexicon and text\n",
    "We perform quite a number of checks. \n",
    "The match should be perfect.\n",
    "If not, then quite possible the MQL core data has been exported at an other time the the lexical data.\n",
    "\n",
    "## Varia\n",
    "1. `lex` contains the lexeme (in transcription) with disambiguation marks (`[/=`) appended.\n",
    "   For text transformations we prefer the bare lexeme\n",
    "1. `lex_utf` has frills at the end of many values. Probably they have arisen by transforming the lexeme plus\n",
    "   disambiguation marks into unicode. We overwrite this feature with the transform of the bare lexeme.\n",
    "1. `language` has values `Hebrew` and `Aramaic`. We prefer ISO language codes: `hbo` and `arc` instead.\n",
    "   By adding `language` for lexeme nodes we already have switched to ISO codes. Here we do the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os,sys,re,collections\n",
    "from tf.fabric import Fabric\n",
    "from tf.transcription import Transcription\n",
    "from utils import startNow, tprint, checkDiffs, deliverFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 'SCRIPT' not in locals():\n",
    "    SCRIPT = False\n",
    "    CORE_NAME = 'bhsa'\n",
    "    VERSION= 'c'\n",
    "    CORE_MODULE ='core' \n",
    "\n",
    "def stop(good=False):\n",
    "    if SCRIPT: sys.exit(0 if good else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the context: source file and target directories\n",
    "\n",
    "The conversion is executed in an environment of directories, so that sources, temp files and\n",
    "results are in convenient places and do not have to be shifted around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "module = CORE_MODULE\n",
    "repoBase = os.path.expanduser('~/github/etcbc')\n",
    "thisRepo = '{}/{}'.format(repoBase, CORE_NAME)\n",
    "\n",
    "thisSource = '{}/source/{}'.format(thisRepo, VERSION)\n",
    "\n",
    "thisTemp = '{}/_temp/{}'.format(thisRepo, VERSION)\n",
    "thisSave = '{}/{}'.format(thisTemp, module)\n",
    "\n",
    "thisTf = '{}/tf/{}'.format(thisRepo, VERSION)\n",
    "thisDeliver = '{}/{}'.format(thisTf, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testFeature = 'lex0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Check whether this conversion is needed in the first place.\n",
    "Only when run as a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if SCRIPT:\n",
    "    (good, work) = MUSTRUN(None, '{}/.tf/{}.tfx'.format(thisDeliver, testFeature))\n",
    "    if not good: stop(good=False)\n",
    "    if not work: stop(good=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Settings\n",
    "\n",
    "* a piece of metadata that will go into these features; the time will be added automatically\n",
    "* new text formats for the `otext` feature of TF, based on lexical features.\n",
    "\n",
    "We do not do this for the older versions 4 and 4b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "provenanceMetadata = dict(\n",
    "    dataset='BHSA',\n",
    "    datasetName='Biblia Hebraica Stuttgartensia Amstelodamensis',\n",
    "    author='Eep Talstra Centre for Bible and Computer',\n",
    "    encoders='Constantijn Sikkel (QDF), and Dirk Roorda (TF)',\n",
    "    website='https://shebanq.ancient-data.org',\n",
    "    email='shebanq@ancient-data.org',\n",
    ")\n",
    "\n",
    "lexType = 'lex'\n",
    "\n",
    "oText = {\n",
    "    'c': '''\n",
    "@fmt:lex-trans-plain={lex0} \n",
    "''',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function selects the proper otext material, falling back on a default if nothing \n",
    "appropriate has been specified in `oText`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getOtext():\n",
    "    thisOtext = oText.get(VERSION, '')\n",
    "    otextInfo = dict(line[1:].split('=', 1) for line in thisOtext.strip('\\n').split('\\n'))\n",
    "\n",
    "    if thisOtext is '':\n",
    "        print('No additional text formats provided') \n",
    "    else:\n",
    "        print('New text formats')\n",
    "    for x in sorted(otextInfo.items()):\n",
    "        print('{:<20} = \"{}\"'.format(*x))\n",
    "    return otextInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Lexicon preparation\n",
    "We add lexical data.\n",
    "The lexical data will not be added as features of words, but as features of lexemes.\n",
    "The lexemes will be added as fresh nodes, of a new type `lex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.12\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "95 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.53s B oslots               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.12s B lex                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.19s B lex_utf8             from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.15s B language             from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.16s B sp                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.15s B ls                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.13s B gn                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.15s B ps                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.15s B nu                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.12s B st                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.16s B g_voc_lex            from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.22s B g_voc_lex_utf8       from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s Feature overview: 90 for nodes; 4 for edges; 1 configs; 7 computed\n",
      "  5.72s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations=thisTf, modules=module)\n",
    "api = TF.load('lex lex_utf8 language sp ls gn ps nu st g_voc_lex g_voc_lex_utf8 oslots')\n",
    "F = api.F\n",
    "Fs = api.Fs\n",
    "E = api.E\n",
    "T = api.T\n",
    "N = api.N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pass\n",
    "We map the values in the language feature to standardized iso values: `arc` and `hbo`.\n",
    "We run over all word occurrences, grab the language and lexeme identifier, and create for each\n",
    "unique pair a new lexeme node.\n",
    "\n",
    "We remember the mapping between nodes and lexemes.\n",
    "\n",
    "This stage does not yet involve the lexical files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 9236 lexemes\n",
      "maxNode is now 1446130\n"
     ]
    }
   ],
   "source": [
    "langMap = {\n",
    "    'hbo': 'hbo',\n",
    "    'Hebrew': 'hbo',\n",
    "    'Aramaic': 'arc',\n",
    "    'arc': 'arc',\n",
    "}\n",
    "\n",
    "maxNode = F.otype.maxNode\n",
    "maxSlot = F.otype.maxSlot\n",
    "slotType = F.otype.slotType\n",
    "\n",
    "lexNode = maxNode\n",
    "lexOccs = {}\n",
    "nodeFromLex = {}\n",
    "lexFromNode = {}\n",
    "otypeData = {}\n",
    "oslotsData = {}\n",
    "\n",
    "for n in F.otype.s('word'):\n",
    "    lex = F.lex.v(n)\n",
    "    lan = langMap[F.language.v(n)]\n",
    "    lexId = (lan, lex)\n",
    "    lexOccs.setdefault(lexId, []).append(n)\n",
    "    if lexId not in nodeFromLex:\n",
    "        lexNode += 1\n",
    "        nodeFromLex[lexId] = lexNode\n",
    "        lexFromNode[lexNode] = lexId\n",
    "print('added {} lexemes\\nmaxNode is now {}'.format(len(nodeFromLex), lexNode))\n",
    "\n",
    "for n in range(maxNode+1, lexNode+1):\n",
    "    otypeData[n] = 'lex'\n",
    "    oslotsData[n] = lexOccs[lexFromNode[n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon pass\n",
    "Here we are going to read the lexicons, one for Aramaic, and one for Hebrew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lexicon ...\n",
      "Lexicon arc has   708 entries\n",
      "Lexicon hbo has  8528 entries\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "langs = set(langMap.values())\n",
    "lexFile = dict((lan, '{}/lexicon_{}.txt'.format(thisSource, lan)) for lan in langs)\n",
    "\n",
    "def readLex(lan):\n",
    "    lexInfile = open(lexFile[lan], encoding='utf-8')\n",
    "    errors = []\n",
    "\n",
    "    lexItems = {}\n",
    "    ln = 0\n",
    "    for line in lexInfile:\n",
    "        ln += 1\n",
    "        line = line.rstrip()\n",
    "        line = line.split('#')[0]\n",
    "        if line == '': continue\n",
    "        (entry, featurestr) = line.split(sep=None, maxsplit=1)\n",
    "        entry = entry.strip('\"')\n",
    "        if entry in lexItems:\n",
    "            errors.append('duplicate lexical entry {} in line {}.\\n'.format(entry, ln))\n",
    "            continue\n",
    "        featurestr = featurestr.strip(':')\n",
    "        featurestr = featurestr.replace('\\\\:', chr(254))\n",
    "        featurelst = featurestr.split(':')\n",
    "        features = {}\n",
    "        for feature in featurelst:\n",
    "            comps = feature.split('=', maxsplit=1)\n",
    "            if len(comps) == 1:\n",
    "                if feature.strip().isnumeric():\n",
    "                    comps = ('_n', feature.strip())\n",
    "                else:\n",
    "                    errors.append('feature without value for lexical entry {} in line {}: {}\\n'.format(\n",
    "                            entry, ln, feature,\n",
    "                    ))\n",
    "                    continue\n",
    "            (key, value) = comps\n",
    "            value = value.replace(chr(254), ':')\n",
    "            if key in features:\n",
    "                errors.append('duplicate feature for lexical entry {} in line {}: {}={}\\n'.format(\n",
    "                        entry, ln, key, value,\n",
    "                ))\n",
    "                continue\n",
    "            features[key] = value.replace('\\\\', '/')\n",
    "        if 'sp' in features and features['sp'] == 'verb':\n",
    "            if 'gl' in features:\n",
    "                gloss = features['gl']\n",
    "                if gloss.startswith('to '):\n",
    "                    features['gl'] = gloss[3:]\n",
    "        lexItems[entry] = features\n",
    "        \n",
    "    lexInfile.close()\n",
    "    nErrors = len(errors)\n",
    "    if len(errors):\n",
    "        print('Lexicon [{}]: {} error{}'.format(nErrors, '' if nErrors == 1 else 's'))\n",
    "    return lexItems\n",
    "\n",
    "print(\"Reading lexicon ...\")\n",
    "lexEntries = dict((lan, readLex(lan)) for lan in sorted(langs))\n",
    "for lan in sorted(lexEntries):\n",
    "    print('Lexicon {} has {:>5} entries'.format(lan, len(lexEntries[lan])))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "## Matching of text and lexicon\n",
    "\n",
    "We inspect all word occurrences of the BHSA core database, inspect their language and lexeme values, and construct sets of lexemes that belong to each of the two languages, ``hbo`` and ``arc``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the BHSA core data ...\n",
      "Done\n",
      "Language arc has   708 lexemes in the text\n",
      "Language hbo has  8528 lexemes in the text\n"
     ]
    }
   ],
   "source": [
    "lexText = {}\n",
    "doValueCompare = {'sp', 'ls', 'gn', 'ps', 'nu', 'st'}\n",
    "nodeLex = {}\n",
    "\n",
    "print('Reading the BHSA core data ...')\n",
    "textLangs = set()\n",
    "for n in F.otype.s('word'):\n",
    "    lan = langMap[F.language.v(n)]\n",
    "    textLangs.add(lan)\n",
    "    lex = F.lex.v(n)\n",
    "    nodeLex[n] = (lan,lex)\n",
    "    for ft in doValueCompare:\n",
    "        val = Fs(ft).v(n)        \n",
    "        lexText.setdefault(lan, {}).setdefault(lex, {}).setdefault(ft, set()).add(val)\n",
    "\n",
    "print(\"Done\")\n",
    "for lan in sorted(lexText):\n",
    "    print('Language {} has {:>5} lexemes in the text'.format(lan, len(lexText[lan])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now check whether all lexemes in the text occur in the lexicon and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708 arc lexemes\n",
      "8528 hbo lexemes\n",
      "Equal lex values in hbo and arc in the BHSA   text contains 460 lexemes\n",
      "Equal lex values in hbo and arc in the lexicon     contains 460 lexemes\n",
      "Common values in the lexicon but not in the text: 0x: set()\n",
      "Common values in the text but not in the lexicon: 0x: set()\n",
      "arc: lexemes in text but not in lexicon: 0x\n",
      "arc: lexemes in lexicon but not in text: 0x\n",
      "hbo: lexemes in text but not in lexicon: 0x\n",
      "hbo: lexemes in lexicon but not in text: 0x\n"
     ]
    }
   ],
   "source": [
    "arcLex = set(lexEntries['arc'])\n",
    "hboLex = set(lexEntries['hbo'])\n",
    "\n",
    "print('{} arc lexemes'.format(len(arcLex)))\n",
    "print('{} hbo lexemes'.format(len(hboLex)))\n",
    "\n",
    "arcText = set(lexText['arc'])\n",
    "hboText = set(lexText['hbo'])\n",
    "\n",
    "hboAndArcText = arcText & hboText\n",
    "hboAndArcLex = arcLex & hboLex\n",
    "\n",
    "lexMinText = hboAndArcLex - hboAndArcText\n",
    "textMinLex = hboAndArcText - hboAndArcLex\n",
    "\n",
    "print('Equal lex values in hbo and arc in the BHSA   text contains {} lexemes'.format(len(hboAndArcText)))\n",
    "print('Equal lex values in hbo and arc in the lexicon     contains {} lexemes'.format(len(hboAndArcLex)))\n",
    "print(\"Common values in the lexicon but not in the text: {}x: {}\".format(\n",
    "    len(lexMinText), lexMinText)\n",
    ")\n",
    "print(\"Common values in the text but not in the lexicon: {}x: {}\".format(\n",
    "    len(textMinLex), textMinLex)\n",
    ")\n",
    "\n",
    "arcTextMinLex = arcText - arcLex\n",
    "arcLexMinText = arcLex - arcText\n",
    "\n",
    "hboTextMinLex = hboText - hboLex\n",
    "hboLexMinText = hboLex - hboText\n",
    "\n",
    "for (myset, mymsg) in (\n",
    "    (arcTextMinLex, 'arc: lexemes in text but not in lexicon'),\n",
    "    (arcLexMinText, 'arc: lexemes in lexicon but not in text'),\n",
    "    (hboTextMinLex, 'hbo: lexemes in text but not in lexicon'),\n",
    "    (hboLexMinText, 'hbo: lexemes in lexicon but not in text'),\n",
    "):\n",
    "    print('{}: {}x{}'.format(mymsg, len(myset), '' if not myset else '\\n\\t{}'.format(', '.join(sorted(myset)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency of vocalized lexeme\n",
    "\n",
    "The lexicon file provides an attribute `vc` for each lexeme, which is the vocalized lexeme.\n",
    "The ETCBC core data also has features `g_voc_lex` and `g_voc_lex_utf8` for each occurrence.\n",
    "\n",
    "We investigate whether the latter features are *consistent*, i.e. a property of the lexeme and lexeme only.\n",
    "If they are somehow dependent on the word occurrence, they are not consistent.\n",
    "\n",
    "When they are consistent, we can omit them on the occurrences and use them on the lexemes.\n",
    "We'll also check whether the `vc` property found in the lexicon coincides with the `g_voc_lex` on the occurrences.\n",
    "\n",
    "Supposing it is all consistent, we will call the new lexeme features `voc_lex` and `voc_lex_utf8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexemes with missing vc property: 343x\n",
      "\thbo-<BC[ supplied from occurrence: <BC\n",
      "\thbo-<BR=[ supplied from occurrence: <BR\n",
      "\thbo-<BT[ supplied from occurrence: <BT\n",
      "\thbo-<BV=[ supplied from occurrence: <BV\n",
      "\thbo-<DN[ supplied from occurrence: <DN\n",
      "\thbo-<DP[ supplied from occurrence: <DP\n",
      "\thbo-<DR==[ supplied from occurrence: <DR\n",
      "\thbo-<DR=[ supplied from occurrence: <DR\n",
      "\thbo-<DR[ supplied from occurrence: <DR\n",
      "\thbo-<FQ[ supplied from occurrence: <FQ\n",
      "\thbo-<FR[ supplied from occurrence: <FR\n",
      "\thbo-<GB[ supplied from occurrence: <GB\n",
      "\thbo-<GM[ supplied from occurrence: <GM\n",
      "\thbo-<GN[ supplied from occurrence: <GN\n",
      "\thbo-<KS[ supplied from occurrence: <KS\n",
      "\thbo-<L<[ supplied from occurrence: <L<\n",
      "\thbo-<LS[ supplied from occurrence: <LS\n",
      "\thbo-<MR[ supplied from occurrence: <MR\n",
      "\thbo-<MS[ supplied from occurrence: <MS\n",
      "\thbo-<ND[ supplied from occurrence: <ND\n",
      "Have all occurrences of a lexeme the same voc_lex value?\n",
      "Fully consistent\n",
      "Have all occurrences of a lexeme the same voc_lex_utf8 value?\n",
      "Fully consistent\n",
      "Are the voc_lex values of the lexeme consistent with the vc value of the lexeme?\n",
      "1418 inconsistent cases\n",
      "hbo-BR>[: B.@R@>, BR>\n",
      "hbo-HJH[: H@J@H, HJH\n",
      "hbo-RXP[: R@XAP, RXP\n",
      "hbo->MR[: >@MAR, >MR\n",
      "hbo-R>H[: R>H, R@>@H\n",
      "hbo-VWB[: VOWB, VWB\n",
      "hbo-BDL[: B.@DAL, BDL\n",
      "hbo-QR>[: Q@R@>, QR>\n",
      "hbo-<FH[: <@F@H, <FH\n",
      "hbo-QWH=[: Q@W@H, QWH\n",
      "...and 1408 more.\n"
     ]
    }
   ],
   "source": [
    "vocFeatures = dict(voc_lex={}, voc_lex_utf8={})\n",
    "\n",
    "exceptions = dict(incons=dict((f, {}) for f in vocFeatures), deviating=dict())\n",
    "\n",
    "missing = {}\n",
    "\n",
    "def showExceptions(cases):\n",
    "    nCases = len(cases)\n",
    "    if nCases == 0:\n",
    "        print('Fully consistent')\n",
    "    else:\n",
    "        print('{} inconsistent cases'.format(nCases))\n",
    "        limit = 10\n",
    "        for (i, (lan, lex)) in enumerate(cases):\n",
    "            if i == limit:\n",
    "                print('...and {} more.'.format(nCases - limit))\n",
    "                break\n",
    "            print('{}-{}: {}'.format(lan, lex, ', '.join(sorted(cases[(lan, lex)]))))\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    lan = langMap[F.language.v(w)]\n",
    "    lex = F.lex.v(w)\n",
    "    for (f, values) in vocFeatures.items():\n",
    "        current = values.get((lan, lex), None)\n",
    "        new = Fs('g_{}'.format(f)).v(w)\n",
    "        if current == None:\n",
    "            values[(lan, lex)] = new\n",
    "            if f == 'voc_lex':\n",
    "                lexical = lexEntries[lan][lex].get('vc', None)\n",
    "                if lexical == None:\n",
    "                    missing[(lan, lex)] = new\n",
    "                else:\n",
    "                    if lexical != new: exceptions['deviating'].setdefault((lan, lex), {lexical}).add(new)\n",
    "        else:\n",
    "            if current != new:\n",
    "                exceptions['incons'][f].setdefault((lan, lex), {current}).add(new)\n",
    "\n",
    "nMissing = len(missing)\n",
    "print('lexemes with missing vc property: {}x'.format(nMissing))\n",
    "for (lan, lex) in sorted(missing)[0:20]:\n",
    "    print('\\t{}-{} supplied from occurrence: {}'.format(lan, lex, vocFeatures['voc_lex'][(lan, lex)]))\n",
    "for f in vocFeatures:\n",
    "    print('Have all occurrences of a lexeme the same {} value?'.format(f))\n",
    "    showExceptions(exceptions['incons'][f])\n",
    "print('Are the voc_lex values of the lexeme consistent with the vc value of the lexeme?')\n",
    "showExceptions(exceptions['deviating'])                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare TF features\n",
    "\n",
    "We now collect the lexical information into the features for nodes of type `lex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeFeatures = {}\n",
    "\n",
    "lexFields = (\n",
    "    ('rt', 'root'),\n",
    "    ('sp', 'sp'),\n",
    "    ('sm', 'nametype'),\n",
    "    ('ls', 'ls'),\n",
    "    ('gl', 'gloss'),\n",
    ")\n",
    "\n",
    "overlapFeatures = {'lex', 'language', 'sp', 'ls'} # both on word- and lex- otypes\n",
    "\n",
    "for f in overlapFeatures:\n",
    "    nodeFeatures[f] = dict((n, Fs(f).v(n)) for n in N() if Fs(f).v(n) != None)\n",
    "\n",
    "newFeatures = [f[1] for f in lexFields]\n",
    "\n",
    "for (lan, lexemes) in lexEntries.items():\n",
    "    for (lex, lexValues) in lexemes.items():\n",
    "        node = nodeFromLex[(lan, lex)]\n",
    "        nodeFeatures.setdefault('lex', {})[node] = lex\n",
    "        nodeFeatures.setdefault('language', {})[node] = lan\n",
    "        for (f, newF) in lexFields:\n",
    "            value = lexValues.get(f, None)\n",
    "            if value != None:\n",
    "                nodeFeatures.setdefault(newF, {})[node] = value\n",
    "        for (f, vocValues) in vocFeatures.items():\n",
    "            value = vocValues.get((lan, lex), None)\n",
    "            if value != None:\n",
    "                nodeFeatures.setdefault(f, {})[node] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We address the issues listed under varia above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeFeatures['lex0'] = {}\n",
    "nodeFeatures['lex_utf8'] = {}\n",
    "\n",
    "for n in F.otype.s('word'):\n",
    "    lex = F.lex.v(n).rstrip('[/=')\n",
    "    lan = F.language.v(n)\n",
    "    nodeFeatures['lex0'][n] = lex\n",
    "    nodeFeatures['lex_utf8'][n] = Transcription.to_hebrew(lex)\n",
    "    nodeFeatures['language'][n] = langMap[lan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['בְּ', 'רֵאשִׁית', 'ברא', 'אֱלֹהִים', 'אֵת', 'הַ', 'שָׁמַיִם', 'וְ', 'אֶרֶץ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testNodes = range(maxNode+1, maxNode + 10)\n",
    "[nodeFeatures['voc_lex_utf8'][n] for n in testNodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the `otype`, `otext` and `oslots` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New text formats\n",
      "fmt:lex-trans-plain  = \"{lex0} \"\n"
     ]
    }
   ],
   "source": [
    "metaData = {}\n",
    "edgeFeatures = {}\n",
    "\n",
    "metaData['otext'] = dict()\n",
    "metaData['otext'].update(T.config)\n",
    "metaData['otext'].update(getOtext())\n",
    "metaData['otype'] = dict(valueType='str')\n",
    "metaData['oslots'] = dict(valueType='str')\n",
    "\n",
    "for f in nodeFeatures:\n",
    "    metaData[f] = {}\n",
    "    metaData[f].update(provenanceMetadata)\n",
    "    metaData[f]['valueType'] = 'str'\n",
    "\n",
    "nodeFeatures['otype'] = dict((n, F.otype.v(n)) for n in range(1, maxNode +1))\n",
    "nodeFeatures['otype'].update(otypeData)\n",
    "edgeFeatures['oslots'] = dict((n, E.oslots.s(n)) for n in range(maxSlot + 1, maxNode +1))\n",
    "edgeFeatures['oslots'].update(oslotsData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the features to delete and list the new/changed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deleteFeatures = set('''\n",
    "    g_voc_lex\n",
    "    g_voc_lex_utf8\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "changedDataFeatures = set(nodeFeatures) | set(edgeFeatures)\n",
    "changedFeatures = changedDataFeatures | {'otext'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: TF generation\n",
    "Transform the collected information in feature-like datastructures, and write it all\n",
    "out to `.tf` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00s write new/changed features to TF ...\n",
      "This is Text-Fabric 2.3.12\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "0 features found and 0 ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s Grid feature \"otype\" not found in\n",
      "\n",
      "  0.00s Grid feature \"oslots\" not found in\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Grid feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Exporting 12 node and 1 edge and 1 config features to /Users/dirk/github/etcbc/bhsa/_temp/c/core:\n",
      "   |     0.03s T gloss                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.67s T language             to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.67s T lex                  to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.76s T lex0                 to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.89s T lex_utf8             to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.78s T ls                   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.01s T nametype             to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.66s T otype                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s T root                 to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.68s T sp                   to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.03s T voc_lex              to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.03s T voc_lex_utf8         to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     4.16s T oslots               to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "   |     0.00s M otext                to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n",
      "  9.42s Exported 12 node features and 1 edge features and 1 config features to /Users/dirk/github/etcbc/bhsa/_temp/c/core\n"
     ]
    }
   ],
   "source": [
    "def tfFromData():\n",
    "    startNow()\n",
    "    tprint('write new/changed features to TF ...')\n",
    "    TF = Fabric(locations=thisSave)\n",
    "    TF.save(nodeFeatures=nodeFeatures, edgeFeatures=edgeFeatures, metaData=metaData)\n",
    "\n",
    "tfFromData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Diffs\n",
    "\n",
    "Check differences with previous versions.\n",
    "\n",
    "The new dataset has been created in a temporary directory,\n",
    "and has not yet been copied to its destination.\n",
    "\n",
    "Here is your opportunity to compare the newly created features with the older features.\n",
    "You expect some differences in some features.\n",
    "\n",
    "We check the differences between the previous version of the features and what has been generated.\n",
    "We list features that will be added and deleted and changed.\n",
    "For each changed feature we show the first line where the new feature differs from the old one.\n",
    "We ignore changes in the metadata, because the timestamp in the metadata will always change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00s checkDiffs\n",
      "6 features to add:\n",
      "\tgloss lex0 nametype root voc_lex voc_lex_utf8\n",
      "no features to delete\n",
      "8 features in common\n",
      "language                  ... First diff in line 2 after the metadata\n",
      "OLD -->Hebrew\n",
      "<--\n",
      "NEW -->hbo\n",
      "<--\n",
      "First diff in line 3 after the metadata\n",
      "OLD -->Hebrew\n",
      "<--\n",
      "NEW -->hbo\n",
      "<--\n",
      "\n",
      "lex                       ... First diff in line 426583 after the metadata\n",
      "OLD --><empty><--\n",
      "NEW -->1436895\tB\n",
      "<--\n",
      "\n",
      "lex_utf8                  ... First diff in line 3 after the metadata\n",
      "OLD -->ראשׁית֜\n",
      "<--\n",
      "NEW -->ראשׁית\n",
      "<--\n",
      "\n",
      "ls                        ... First diff in line 426583 after the metadata\n",
      "OLD --><empty><--\n",
      "NEW -->1436904\tvbcp\n",
      "<--\n",
      "\n",
      "oslots                    ... First diff in line 1010315 after the metadata\n",
      "OLD --><empty><--\n",
      "NEW -->1,84,197,220,241,270,318,330,334,428,435,500,506,5<--\n",
      "\n",
      "otext                     ... First diff in line 5 \n",
      "OLD -->@email=shebanq@ancient-data.org\n",
      "<--\n",
      "NEW -->@dateWritten=2017-09-27T23:02:24Z\n",
      "<--\n",
      "\n",
      "otype                     ... First diff in line 14 after the metadata\n",
      "OLD --><empty><--\n",
      "NEW -->1436895-1446130\tlex\n",
      "<--\n",
      "\n",
      "sp                        ... First diff in line 426583 after the metadata\n",
      "OLD --><empty><--\n",
      "NEW -->1436895\tprep\n",
      "<--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkDiffs(thisSave, thisDeliver, only=changedFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Deliver \n",
    "\n",
    "Copy the new TF dataset from the temporary location where it has been created to its final destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy features from /Users/dirk/github/etcbc/bhsa/_temp/c/core to /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "lex0\n",
      "otype\n",
      "lex_utf8\n",
      "sp\n",
      "ls\n",
      "voc_lex\n",
      "nametype\n",
      "voc_lex_utf8\n",
      "language\n",
      "oslots\n",
      "root\n",
      "gloss\n",
      "lex\n",
      "otext\n",
      "Delete features from /Users/dirk/github/etcbc/bhsa/tf/c/core\n"
     ]
    }
   ],
   "source": [
    "deliverFeatures(thisSave, thisDeliver, changedFeatures, deleteFeatures=deleteFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Compile TF\n",
    "\n",
    "We load the new features, use the new format, check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00s load features\n",
      "This is Text-Fabric 2.3.12\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "99 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.94s T otype                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |       12s T oslots               from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.61s T lex0                 from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.71s T lex_utf8             from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |      |     1.29s C __levels__           from otype, oslots\n",
      "   |      |       18s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.97s C __rank__             from otype, __order__\n",
      "   |      |       19s C __levUp__            from otype, oslots, __rank__\n",
      "   |      |       11s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     4.10s C __boundary__         from otype, oslots, __rank__\n",
      "   |     0.00s M otext                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |      |     0.13s C __sections__         from otype, oslots, otext, __levUp__, __levels__, book, chapter, verse\n",
      "   |     1.49s T sp                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.56s T ls                   from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.04s T voc_lex              from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.01s T nametype             from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.04s T voc_lex_utf8         from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.44s T language             from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.01s T root                 from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.04s T gloss                from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     1.47s T lex                  from /Users/dirk/github/etcbc/bhsa/tf/c/core\n",
      "   |     0.00s Feature overview: 94 for nodes; 4 for edges; 1 configs; 7 computed\n",
      " 1m 19s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "startNow()\n",
    "tprint('load features')\n",
    "TF = Fabric(locations=thisTf, modules=module)\n",
    "api = TF.load(' '.join(changedDataFeatures))\n",
    "F = api.F\n",
    "Fs = api.Fs\n",
    "T = api.T\n",
    "L = api.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new format (using lex0)\n",
      "B R>CJT BR> >LHJM >T H CMJM W >T H >RY \n",
      "lex_utf8 feature\n",
      "ב ראשׁית ברא אלהימ את ה שׁמימ ו את ה ארצ\n",
      "language feature\n",
      "hbo hbo hbo hbo hbo hbo hbo hbo hbo hbo hbo\n",
      "\thbo - B - 15542x\n",
      "\t\tgloss           = in\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = None\n",
      "\t\tsp              = prep\n",
      "\t\tvoc_lex         = B.:\n",
      "\t\tvoc_lex_utf8    = בְּ\n",
      "\thbo - R>CJT/ - 51x\n",
      "\t\tgloss           = beginning\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = R>C\n",
      "\t\tsp              = subs\n",
      "\t\tvoc_lex         = R;>CIJT\n",
      "\t\tvoc_lex_utf8    = רֵאשִׁית\n",
      "\thbo - BR>[ - 48x\n",
      "\t\tgloss           = create\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = None\n",
      "\t\tsp              = verb\n",
      "\t\tvoc_lex         = BR>\n",
      "\t\tvoc_lex_utf8    = ברא\n",
      "\thbo - >LHJM/ - 2601x\n",
      "\t\tgloss           = god(s)\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = None\n",
      "\t\tsp              = subs\n",
      "\t\tvoc_lex         = >:ELOHIJM\n",
      "\t\tvoc_lex_utf8    = אֱלֹהִים\n",
      "\thbo - >T - 11002x\n",
      "\t\tgloss           = <object marker>\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = None\n",
      "\t\tsp              = prep\n",
      "\t\tvoc_lex         = >;T\n",
      "\t\tvoc_lex_utf8    = אֵת\n",
      "\thbo - H - 30384x\n",
      "\t\tgloss           = the\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = None\n",
      "\t\tsp              = art\n",
      "\t\tvoc_lex         = HA\n",
      "\t\tvoc_lex_utf8    = הַ\n",
      "\thbo - CMJM/ - 421x\n",
      "\t\tgloss           = heavens\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = CMH\n",
      "\t\tsp              = subs\n",
      "\t\tvoc_lex         = C@MAJIM\n",
      "\t\tvoc_lex_utf8    = שָׁמַיִם\n",
      "\thbo - W - 50272x\n",
      "\t\tgloss           = and\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = None\n",
      "\t\tsp              = conj\n",
      "\t\tvoc_lex         = W:\n",
      "\t\tvoc_lex_utf8    = וְ\n",
      "\thbo - >T - 11002x\n",
      "\t\tgloss           = <object marker>\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = None\n",
      "\t\tsp              = prep\n",
      "\t\tvoc_lex         = >;T\n",
      "\t\tvoc_lex_utf8    = אֵת\n",
      "\thbo - H - 30384x\n",
      "\t\tgloss           = the\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = None\n",
      "\t\tsp              = art\n",
      "\t\tvoc_lex         = HA\n",
      "\t\tvoc_lex_utf8    = הַ\n",
      "\thbo - >RY/ - 2504x\n",
      "\t\tgloss           = earth\n",
      "\t\tls              = None\n",
      "\t\tnametype        = None\n",
      "\t\troot            = None\n",
      "\t\tsp              = subs\n",
      "\t\tvoc_lex         = >EREY\n",
      "\t\tvoc_lex_utf8    = אֶרֶץ\n"
     ]
    }
   ],
   "source": [
    "features = [f[1] for f in lexFields] + ['voc_lex', 'voc_lex_utf8']\n",
    "\n",
    "def printLex(w):\n",
    "    info = dict((f, Fs(f).v(w)) for f in features)\n",
    "    print('\\t{} - {} - {}x'.format(\n",
    "        F.language.v(w),\n",
    "        F.lex.v(w),\n",
    "        len(L.d(w, otype='word'))\n",
    "    ))\n",
    "    for f in sorted(info):\n",
    "        print('\\t\\t{:<15} = {}'.format(f, info[f]))\n",
    "\n",
    "print('new format (using lex0)')\n",
    "print(T.text(range(1,12), fmt='lex-trans-plain'))\n",
    "print('lex_utf8 feature')\n",
    "print(' '.join(F.lex_utf8.v(w) for w in range(1,12)))\n",
    "print('language feature')\n",
    "print(' '.join(F.language.v(w) for w in range(1,12)))\n",
    "for w in range(1, 12): printLex(L.u(w, otype='lex')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
