{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"tf-small.png\"/>\n",
    "\n",
    "# TF from MQL\n",
    "\n",
    "This notebook can read an\n",
    "[MQL](https://emdros.org/mql.html)\n",
    "dump of a version of the [BHSA](https://github.com/ETCBC/bhsa) Hebrew Text Database\n",
    "and transform it in a Text-Fabric\n",
    "[Text-Fabric](https://github.com/ETCBC/text-fabric)\n",
    "resource.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The principled way of going about such a conversion is to import the MQL source into\n",
    "an [Emdros](https://emdros.org) database, and use it to retrieve objects and features from there.\n",
    "\n",
    "Because the syntax of an MQL file leaves some freedom, it is error prone to do a text-to-text conversion from\n",
    "MQL to something else.\n",
    "\n",
    "Yet this is what we do, the error-prone thing. We then avoid installing and configuring and managing Emdros, MySQL/sqLite3.\n",
    "Aside the upfront work to get this going, the going after that is also much slower.\n",
    "\n",
    "So here you are, a smallish script to do an awful lot of work, mostly correct, if careful used.\n",
    "\n",
    "# Caveat\n",
    "\n",
    "This notebook makes use of a new feature of text-fabric, first present in 2.3.12.\n",
    "Make sure to upgrade first.\n",
    "\n",
    "```sudo -H pip3 install text-fabric```\n",
    "\n",
    "# Temporary hack\n",
    "The ETCBC does not yet produce an MQL file that satisfies all the requirements.\n",
    "Some features are still missing.\n",
    "In the etcbc4c version I have worked around those details, which resulted in a complete dataset 4c.\n",
    "\n",
    "In order to test the later modules dependent on the ETCBC data, we use this version, and call it `d`\n",
    "(development purposes).\n",
    "\n",
    "Hopefull we can agree on these requirements for the future continuous `c` version and the fixed versions `2017` etc.\n",
    "\n",
    "So, in this notebook, if the version is `d`, we skip the mql version, and copy over the datasource straight from\n",
    "the text-fabric-data/hebrew/4c directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os,sys,re,collections\n",
    "from shutil import rmtree, copytree\n",
    "from tf.fabric import Fabric\n",
    "from utils import bunzip, startNow, tprint, checkDiffs\n",
    "from blang import bookLangs, bookNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modes of execution\n",
    "\n",
    "This notebook is meant to operate in two modes, *notebook* and *script*.\n",
    "\n",
    "## Notebook mode\n",
    "Meant for interactively running this script. \n",
    "The MQL parsing might incur errors, due to varying patterns in the\n",
    "MQL dump. \n",
    "If that happens, you need some interactive tweaking of the `parseMQL()` function.\n",
    "Or, if there are issues with the `tfFromData()` function, it is handy that\n",
    "the results of `parseMQL()` stay in memory, while debugging `tfFromData()`.\n",
    "\n",
    "In notebook mode, the parameters are just the capitalized variables.\n",
    "\n",
    "## Script mode\n",
    "The task of converting MQL to TF is part of the data pipeline from the ETCBC institute to the SHEBANQ website.\n",
    "Therefore this script must be able to run unsupervised, from another python program.\n",
    "\n",
    "Here is how that othe program should call this notebook:\n",
    "\n",
    "* convert the script to python with nbconvert\n",
    "* read the script as file and execute it as a python string, supplying\n",
    "  values for the capitalized variables as local variables\n",
    "* one of those variables is: `SCRIPT=True`\n",
    "\n",
    "The notebook should be written in such a way, that when `SCRIPT=True`,\n",
    "the required conversion will be done and nothing more.\n",
    "\n",
    "We pass the name of the data source, the version, and the name of a target TF module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 'SCRIPT' not in locals():\n",
    "    SCRIPT = False\n",
    "    CORE_NAME = 'bhsa'\n",
    "    VERSION= 'd'\n",
    "    CORE_MODULE ='core' \n",
    "\n",
    "def stop(good=False):\n",
    "    if SCRIPT: sys.exit(0 if good else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the context: source file and target directories\n",
    "\n",
    "The conversion is executed in an environment of directories, so that sources, temp files and\n",
    "results are in convenient places and do not have to be shifted around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "module = CORE_MODULE\n",
    "repoBase = os.path.expanduser('~/github/etcbc')\n",
    "thisRepo = '{}/{}'.format(repoBase, CORE_NAME)\n",
    "\n",
    "thisSource = '{}/source'.format(thisRepo)\n",
    "mqlzFile = '{}/{}-{}.mql.bz2'.format(thisSource, CORE_NAME, VERSION)\n",
    "\n",
    "thisTemp = '{}/_temp/{}'.format(thisRepo, VERSION)\n",
    "mqlFile = '{}/{}-{}.mql'.format(thisTemp, CORE_NAME, VERSION)\n",
    "thisSave = '{}/{}'.format(thisTemp, module)\n",
    "\n",
    "thisTf = '{}/tf/{}'.format(thisRepo, VERSION)\n",
    "thisDeliver = '{}/{}'.format(thisTf, module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Check whether this conversion is needed in the first place.\n",
    "Only when run as a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCRIPT:\n",
    "    testFile = '{}/.tf/otype.tfx'.format(thisDeliver)\n",
    "    if VERSION == 'd':\n",
    "        testFile = '{}/otype.tf'.format(thisDeliver)\n",
    "        test = os.path.exists(testFile)\n",
    "        if test:\n",
    "            print('Dataset in place')\n",
    "        else: \n",
    "            print('Dataset not present. Copy it over from {} to {}'.format(\n",
    "                'text-fabric-data/hebrew/etcbc4c', thisDeliver,\n",
    "            ))\n",
    "        stop(good=test)\n",
    "    (good, work) = MUSTRUN(mqlzFile, '{}/.tf/otype.tfx'.format(thisDeliver))\n",
    "    if not good: stop(good=False)\n",
    "    if not work: stop(good=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Settings\n",
    "\n",
    "We add some custom information here.\n",
    "\n",
    "* the MQL object type that corresponds to the TF slot type, typically `word`;\n",
    "* a piece of metadata that will go into every feature; the time will be added automatically\n",
    "* suitable text formats for the `otext` feature of TF.\n",
    "\n",
    "The oText feature is very sensitive to what is available in the source MQL.\n",
    "It needs to be configured here.\n",
    "We save the configs we need per source and version.\n",
    "And we define a stripped down default version to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "slotType = 'word'\n",
    "\n",
    "featureMetadata = dict(\n",
    "    dataset='BHSA',\n",
    "    datasetName='Biblia Hebraica Stuttgartensia Amstelodamensis',\n",
    "    author='Eep Talstra Centre for Bible and Computer',\n",
    "    encoders='Constantijn Sikkel (QDF), Ulrik Petersen (MQL) and Dirk Roorda (TF)',\n",
    "    website='https://shebanq.ancient-data.org',\n",
    "    email='shebanq@ancient-data.org',\n",
    ")\n",
    "\n",
    "oText = {\n",
    "    '': {\n",
    "        '': '''\n",
    "@sectionFeatures=book,chapter,verse\n",
    "@sectionTypes=book,chapter,verse\n",
    "@fmt:text-orig-full={g_word_utf8}{g_suffix_utf8}\n",
    "        ''',\n",
    "    },\n",
    "    'x_etcbc': {\n",
    "        '4': '''\n",
    "@fmt:lex-orig-full={g_lex_utf8} \n",
    "@fmt:lex-orig-plain={lex_utf8} \n",
    "@fmt:lex-trans-full={g_lex} \n",
    "@fmt:lex-trans-plain={lex} \n",
    "@fmt:text-orig-full={g_qere_utf8/g_word_utf8}{qtrailer_utf8/trailer_utf8}\n",
    "@fmt:text-orig-full-ketiv={g_word_utf8}{trailer_utf8}\n",
    "@fmt:text-orig-plain={g_cons_utf8}{trailer_utf8}\n",
    "@fmt:text-trans-full={g_word} \n",
    "@fmt:text-trans-full-ketiv={g_word} \n",
    "@fmt:text-trans-plain={g_cons} \n",
    "@sectionFeatures=book,chapter,verse\n",
    "@sectionTypes=book,chapter,verse\n",
    "        ''',\n",
    "        '4b': '''\n",
    "@fmt:lex-orig-full={g_lex_utf8} \n",
    "@fmt:lex-orig-plain={lex_utf8} \n",
    "@fmt:lex-trans-full={g_lex} \n",
    "@fmt:lex-trans-plain={lex} \n",
    "@fmt:text-orig-full={g_qere_utf8/g_word_utf8}{qtrailer_utf8/trailer_utf8}\n",
    "@fmt:text-orig-full-ketiv={g_word_utf8}{trailer_utf8}\n",
    "@fmt:text-orig-plain={g_cons_utf8}{trailer_utf8}\n",
    "@fmt:text-trans-full={g_word} \n",
    "@fmt:text-trans-full-ketiv={g_word} \n",
    "@fmt:text-trans-plain={g_cons} \n",
    "@sectionFeatures=book,chapter,verse\n",
    "@sectionTypes=book,chapter,verse\n",
    "        ''',\n",
    "    },\n",
    "    'bhsa': {\n",
    "        'c': '''\n",
    "@fmt:lex-orig-full={g_lex_utf8} \n",
    "@fmt:lex-orig-plain={lex_utf8} \n",
    "@fmt:lex-trans-full={g_lex} \n",
    "@fmt:lex-trans-plain={lex} \n",
    "@fmt:text-orig-full={qere_utf8/g_word_utf8}{qere_trailer_utf8/trailer_utf8}\n",
    "@fmt:text-orig-full-ketiv={g_word_utf8}{trailer_utf8}\n",
    "@fmt:text-orig-plain={g_cons_utf8}{trailer_utf8}\n",
    "@fmt:text-trans-full={qere/g_word}{qere_trailer/trailer}\n",
    "@fmt:text-trans-full-ketiv={g_word}{trailer}\n",
    "@fmt:text-trans-plain={g_cons}{trailer}\n",
    "@sectionFeatures=book,chapter,verse\n",
    "@sectionTypes=book,chapter,verse\n",
    "        ''',\n",
    "        'd': '''\n",
    "@fmt:lex-orig-full={g_lex_utf8} \n",
    "@fmt:lex-orig-plain={lex_utf8} \n",
    "@fmt:lex-trans-full={g_lex} \n",
    "@fmt:lex-trans-plain={lex} \n",
    "@fmt:text-orig-full={qere_utf8/g_word_utf8}{trailer_utf8}\n",
    "@fmt:text-orig-full-ketiv={g_word_utf8}{trailer_utf8}\n",
    "@fmt:text-orig-plain={g_cons_utf8}{trailer_utf8}\n",
    "@fmt:text-trans-full={qere/g_word}{trailer}\n",
    "@fmt:text-trans-full-ketiv={g_word}{trailer}\n",
    "@fmt:text-trans-plain={g_cons}{trailer}\n",
    "@sectionFeatures=book,chapter,verse\n",
    "@sectionTypes=book,chapter,verse\n",
    "        ''',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function selects the proper otext material, falling back on a default if nothing \n",
    "appropriate has been specified in `oText`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getOtext():\n",
    "    thisOtext = oText.get(CORE_NAME, {}).get(VERSION, oText[''][''])\n",
    "    otextInfo = dict(line[1:].split('=', 1) for line in thisOtext.strip().split('\\n'))\n",
    "\n",
    "    if thisOtext is oText['']['']:\n",
    "        print('WARNING: no otext feature info provided, using a meager default value') \n",
    "    else:\n",
    "        print('INFO: otext feature information found')\n",
    "    for x in sorted(otextInfo.items()):\n",
    "        print('{:<20} = \"{}\"'.format(*x))\n",
    "    return otextInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The program has several stages:\n",
    "   \n",
    "1. **prepare** the source (bunzip if needed)\n",
    "1. **parse MQL** and collect information in datastructures\n",
    "1. **transform to TF** write the datastructures as TF features\n",
    "1. **differences** (informational)\n",
    "1. **deliver** the tf data at its destination directory\n",
    "1. **compile** all tf features to binary format\n",
    "\n",
    "Stages **parseMQL** and **transform to TF** communicate with the help of several global variables:\n",
    "\n",
    "* data containers for the MQL kinds of data\n",
    "  * enumerations\n",
    "  * object types\n",
    "  * tables\n",
    "\n",
    "* data containers for the TF features to be generated,\n",
    "  * node features\n",
    "  * edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "objectTypes = dict()\n",
    "tables = dict()\n",
    "\n",
    "edgeF = dict()\n",
    "nodeF = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Prepare\n",
    "\n",
    "Check the source, bunzip it if needed, empty the result directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepare():\n",
    "    global thisoText\n",
    "\n",
    "    startNow()\n",
    "    tprint('bunzipping {} ...'.format(mqlzFile))\n",
    "    bunzip(mqlzFile, mqlFile)\n",
    "    tprint('Done')\n",
    "\n",
    "    if os.path.exists(thisSave): rmtree(thisSave)\n",
    "    os.makedirs(thisSave)\n",
    "\n",
    "    thisoText = getOtext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a monads specification (a comma separated sequence of numbers and number ranges)\n",
    "into a set of integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: MQL parsing\n",
    "Plough through the MQL file and grab all relevant information\n",
    "and put it into the dedicated data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def setFromSpec(spec):\n",
    "    covered = set()\n",
    "    for r_str in spec.split(','):\n",
    "        bounds = r_str.split('-')\n",
    "        if len(bounds) == 1:\n",
    "            covered.add(int(r_str))\n",
    "        else:\n",
    "            b = int(bounds[0])\n",
    "            e = int(bounds[1])\n",
    "            if (e < b): (b, e) = (e, b)\n",
    "            for n in range(b, e+1): covered.add(n)\n",
    "    return covered\n",
    "\n",
    "def parseMql():\n",
    "    tprint('Parsing mql source ...')\n",
    "    fh = open(mqlFile)\n",
    "\n",
    "    curId = None\n",
    "    curEnum = None\n",
    "    curObjectType = None\n",
    "    curTable = None\n",
    "    curObject = None\n",
    "    curValue = None\n",
    "    curFeature = None\n",
    "\n",
    "    STRING_TYPES = {'ascii', 'string'}\n",
    "\n",
    "    enums = dict()\n",
    "\n",
    "    chunkSize = 1000000\n",
    "    inThisChunk = 0\n",
    "\n",
    "    good = True\n",
    "\n",
    "    for (ln, line) in enumerate(fh):\n",
    "        inThisChunk += 1\n",
    "        if inThisChunk == chunkSize:\n",
    "            tprint('\\tline {:>9}'.format(ln + 1))\n",
    "            inThisChunk = 0\n",
    "        if line.startswith('CREATE OBJECTS WITH OBJECT TYPE') or line.startswith('WITH OBJECT TYPE'):\n",
    "            comps = line.rstrip().rstrip(']').split('[', 1)\n",
    "            curTable = comps[1]\n",
    "            print('\\t\\tobjects in {}'.format(curTable))\n",
    "            curObject = None\n",
    "            if not curTable in tables:\n",
    "                tables[curTable] = dict()\n",
    "        elif curEnum != None:\n",
    "            if line.startswith('}'):\n",
    "                curEnum = None\n",
    "                continue\n",
    "            comps = line.strip().rstrip(',').split('=', 1)\n",
    "            comp = comps[0].strip()\n",
    "            words = comp.split()\n",
    "            if words[0] == 'DEFAULT':\n",
    "                enums[curEnum]['default'] = words[1]\n",
    "                value = words[1]\n",
    "            else:\n",
    "                value = words[0]\n",
    "            enums[curEnum]['values'].append(value)\n",
    "        elif curObjectType != None:\n",
    "            if line.startswith(']'):\n",
    "                curObjectType = None\n",
    "                continue\n",
    "            if curObjectType == True:\n",
    "                if line.startswith('['):\n",
    "                    curObjectType = line.rstrip()[1:]\n",
    "                    objectTypes[curObjectType] = dict()\n",
    "                    print('\\t\\totype {}'.format(curObjectType))\n",
    "                    continue\n",
    "            comps = line.strip().rstrip(';').split(':', 1)\n",
    "            feature = comps[0].strip()\n",
    "            fInfo = comps[1].strip()\n",
    "            fCleanInfo = fInfo.replace('FROM SET', '')\n",
    "            fInfoComps = fCleanInfo.split(' ', 1)\n",
    "            fMQLType = fInfoComps[0]\n",
    "            fDefault = fInfoComps[1].strip().split(' ', 1)[1] if len(fInfoComps) == 1 else None\n",
    "            if fDefault != None and fMQLType in STRING_TYPES:\n",
    "                fDefault = fDefault[1:-1]\n",
    "            default = enums.get(fMQLType, {}).get('default', fDefault)\n",
    "            ftype = 'str' if fMQLType in enums else\\\n",
    "                    'int' if fMQLType == 'integer' else\\\n",
    "                    'str' if fMQLType in STRING_TYPES else\\\n",
    "                    'int' if fInfo == 'id_d' else\\\n",
    "                    'str'\n",
    "            isEdge = fMQLType == 'id_d'\n",
    "            if isEdge:\n",
    "                edgeF.setdefault(curObjectType, set()).add(feature)\n",
    "            else:\n",
    "                nodeF.setdefault(curObjectType, set()).add(feature)\n",
    "\n",
    "            objectTypes[curObjectType][feature] = (ftype, default)\n",
    "            print('\\t\\t\\tfeature {} ({}) = {} : {}'.format(feature, ftype, default, 'edge' if isEdge else 'node'))\n",
    "        elif curTable != None:\n",
    "            if curObject != None:\n",
    "                if line.startswith(']'):\n",
    "                    objectType = objectTypes[curTable]\n",
    "                    for (feature, (ftype, default)) in objectType.items():\n",
    "                        if feature not in curObject['feats'] and default != None:\n",
    "                            curObject['feats'][feature] = default\n",
    "                    tables[curTable][curId] = curObject\n",
    "                    curObject = None\n",
    "                    continue\n",
    "                elif line.startswith('['):\n",
    "                    continue\n",
    "                elif line.startswith('FROM MONADS'):\n",
    "                    monads = line.split('=', 1)[1].replace('{', '').replace('}', '').replace(' ','').strip()\n",
    "                    curObject['monads'] = setFromSpec(monads)\n",
    "                elif line.startswith('WITH ID_D'):\n",
    "                    comps = line.replace('[', '').rstrip().split('=', 1)\n",
    "                    curId = int(comps[1])\n",
    "                elif line.startswith('GO'):\n",
    "                    continue\n",
    "                elif line.strip() == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    if curValue != None:\n",
    "                        toBeContinued = not line.rstrip().endswith('\";')\n",
    "                        if toBeContinued:\n",
    "                            curValue += line\n",
    "                        else:\n",
    "                            curValue += line.rstrip().rstrip(';').rstrip('\"')\n",
    "                            curObject['feats'][curFeature] = curValue\n",
    "                            curValue = None\n",
    "                            curFeature = None\n",
    "                        continue\n",
    "                    if ':=' in line:\n",
    "                        (featurePart, valuePart) = line.split('=', 1)\n",
    "                        feature = featurePart[0:-1].strip()\n",
    "                        isText = ':=\"' in line\n",
    "                        toBeContinued = isText and not line.rstrip().endswith('\";')\n",
    "                        if toBeContinued:\n",
    "                            # this happens if a feature value contains a new line\n",
    "                            # we must continue scanning lines until we meet the ned of the value\n",
    "                            curFeature = feature\n",
    "                            curValue = valuePart.lstrip('\"')\n",
    "                        else:\n",
    "                            value = valuePart.rstrip().rstrip(';').strip('\"')\n",
    "                            curObject['feats'][feature] = value\n",
    "                    else:\n",
    "                        tprint('ERROR: line {}: unrecognized line -->{}<--'.format(ln, line))\n",
    "                        good = False\n",
    "                        break\n",
    "            else:\n",
    "                if line.startswith('CREATE OBJECT'):\n",
    "                    curObject = dict(feats=dict(), monads=None)\n",
    "                    curId = None\n",
    "        else:\n",
    "            if line.startswith('CREATE ENUMERATION'):\n",
    "                words = line.split()\n",
    "                curEnum = words[2]\n",
    "                enums[curEnum] = dict(default=None, values=[])\n",
    "                print('\\t\\tenum {}'.format(curEnum))\n",
    "            elif line.startswith('CREATE OBJECT TYPE'):\n",
    "                curObjectType = True\n",
    "    tprint('{} lines parsed'.format(ln + 1))\n",
    "    fh.close()\n",
    "    for table in tables:\n",
    "        print('{} objects of type {}'.format(len(tables[table]), table))\n",
    "    if not good:\n",
    "        stop(good=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: TF generation\n",
    "Transform the collected information in feature-like datastructures, and write it all\n",
    "out to `.tf` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tfFromData():\n",
    "    tprint('Making TF data ...')\n",
    "    \n",
    "    NIL = {'nil', 'NIL', 'Nil'}\n",
    "\n",
    "    tableOrder = [slotType]+[t for t in sorted(tables) if t != slotType]\n",
    "\n",
    "    nodeFromIdd = dict()\n",
    "    iddFromNode = dict()\n",
    "\n",
    "    nodeFeatures = dict()\n",
    "    edgeFeatures = dict()\n",
    "    metaData = dict()\n",
    "\n",
    "    # metadata that ends up in every feature\n",
    "    metaData[''] = featureMetadata\n",
    "\n",
    "    # the config feature otext\n",
    "    metaData['otext'] = thisoText\n",
    "\n",
    "    # multilingual book names\n",
    "    for (langCode, (langEnglish, langName)) in bookLangs.items():\n",
    "        metaData['book@{}'.format(langCode)] = {\n",
    "            'valueType': 'str',\n",
    "            'language': langName,\n",
    "            'languageCode': langCode,\n",
    "            'languageEnglish': langEnglish,\n",
    "        }\n",
    "\n",
    "    tprint('Monad - idd mapping ...')\n",
    "    otype = dict()\n",
    "    for idd in tables.get(slotType, {}):\n",
    "        monad = list(tables[slotType][idd]['monads'])[0]\n",
    "        nodeFromIdd[idd] = monad\n",
    "        iddFromNode[monad] = idd\n",
    "        otype[monad] = slotType\n",
    "\n",
    "    maxSlot = max(nodeFromIdd.values()) if len(nodeFromIdd) else 0\n",
    "    tprint('maxSlot={}'.format(maxSlot))\n",
    "\n",
    "    tprint('Node mapping and otype ...')\n",
    "    node = maxSlot\n",
    "    for t in tableOrder[1:]:\n",
    "        for idd in sorted(tables[t]):\n",
    "            node += 1\n",
    "            nodeFromIdd[idd] = node\n",
    "            iddFromNode[node] = idd\n",
    "            otype[node] = t\n",
    "\n",
    "    nodeFeatures['otype'] = otype\n",
    "    metaData['otype'] = dict(\n",
    "        valueType='str',\n",
    "    )\n",
    "\n",
    "    tprint('oslots ...')\n",
    "    oslots = dict()\n",
    "    for t in tableOrder[1:]:\n",
    "        for idd in tables.get(t, {}):\n",
    "            node = nodeFromIdd[idd]\n",
    "            monads = tables[t][idd]['monads']\n",
    "            oslots[node] = monads\n",
    "    edgeFeatures['oslots'] = oslots\n",
    "    metaData['oslots'] = dict(\n",
    "        valueType='str',\n",
    "    )\n",
    "\n",
    "    tprint('metadata ...')\n",
    "    for t in nodeF:\n",
    "        for f in nodeF[t]:\n",
    "            ftype = objectTypes[t][f][0]\n",
    "            metaData.setdefault(f, {})['valueType'] = ftype\n",
    "    for t in edgeF:\n",
    "        for f in edgeF[t]:\n",
    "            metaData.setdefault(f, {})['valueType'] = 'str'\n",
    "\n",
    "    tprint('features ...')\n",
    "    chunkSize = 100000\n",
    "    for t in tableOrder:\n",
    "        tprint('\\tfeatures from {}s'.format(t))\n",
    "        inThisChunk = 0\n",
    "        for (i, idd) in enumerate(tables.get(t, {})):\n",
    "            inThisChunk += 1\n",
    "            if inThisChunk == chunkSize:\n",
    "                tprint('\\t{:>9} {}s'.format(i + 1, t))\n",
    "                inThisChunk = 0\n",
    "            node = nodeFromIdd[idd]\n",
    "            features = tables[t][idd]['feats']\n",
    "            for (f, v) in features.items():\n",
    "                isEdge = f in edgeF.get(t, set())\n",
    "                if isEdge:\n",
    "                    if v not in NIL:\n",
    "                        edgeFeatures.setdefault(f, {}).setdefault(node, set()).add(nodeFromIdd[int(v)])\n",
    "                else:\n",
    "                    nodeFeatures.setdefault(f, {})[node] = v\n",
    "        tprint('\\t{:>9} {}s'.format(i + 1, t))\n",
    "\n",
    "    tprint('book names ...')\n",
    "    nodeFeatures['book@la'] = nodeFeatures.get('book', {})\n",
    "    bookNodes = sorted(nodeFeatures.get('book', {}))\n",
    "    for (langCode, langBookNames) in bookNames.items():\n",
    "        nodeFeatures['book@{}'.format(langCode)] = dict(zip(bookNodes, langBookNames))\n",
    "\n",
    "    tprint('write data set to TF ...')\n",
    "\n",
    "    TF = Fabric(locations=thisSave)\n",
    "    TF.save(nodeFeatures=nodeFeatures, edgeFeatures=edgeFeatures, metaData=metaData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Diffs\n",
    "\n",
    "Check differences with previous versions.\n",
    "\n",
    "The new dataset has been created in a temporary directory,\n",
    "and has not yet been copied to its destination.\n",
    "\n",
    "Here is your opportunity to compare the newly created features with the older features.\n",
    "You expect some differences in some features.\n",
    "\n",
    "We check the differences between the previous version of the features and what has been generated.\n",
    "We list features that will be added and deleted and changed.\n",
    "For each changed feature we show the first line where the new feature differs from the old one.\n",
    "We ignore changes in the metadata, because the timestamp in the metadata will always change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Deliver \n",
    "\n",
    "Copy the new TF dataset from the temporary location where it has been created to its final destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deliverDataset():\n",
    "    print('Copy data set to {}'.format(thisDeliver))\n",
    "    if os.path.exists(thisDeliver):\n",
    "        rmtree(thisDeliver)\n",
    "    copytree(thisSave, thisDeliver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage: Compile TF\n",
    "\n",
    "Just to see whether everything loads and the precomputing of extra information works out.\n",
    "Moreover, if you want to work with these features, then the precomputing has already been done, and everything is quicker in subsequent runs.\n",
    "\n",
    "We issue load statement to trigger the precomputing of extra data.\n",
    "Note that all features specified text formats in the `otext` config feature,\n",
    "will be loaded, as well as the features for sections.\n",
    "\n",
    "At that point we have access to the full list of features.\n",
    "We grab them and are going to load them all! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compileTfData():\n",
    "    startNow()\n",
    "    tprint('compileTfData')\n",
    "    TF = Fabric(locations=thisTf, modules=module)\n",
    "    api = TF.load('')\n",
    "    allFeatures = TF.explore(silent=False, show=True)\n",
    "    loadableFeatures = allFeatures['nodes'] + allFeatures['edges']\n",
    "    print(' '.join(loadableFeatures))\n",
    "    api = TF.load(loadableFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parseMql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfFromData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0.00s checkDiffs\n",
      "no features to add\n",
      "no features to delete\n",
      "99 features in common\n",
      "book                      ... no changes\n",
      "book@am                   ... no changes\n",
      "book@ar                   ... no changes\n",
      "book@bn                   ... no changes\n",
      "book@da                   ... no changes\n",
      "book@de                   ... no changes\n",
      "book@el                   ... no changes\n",
      "book@en                   ... no changes\n",
      "book@es                   ... no changes\n",
      "book@fa                   ... no changes\n",
      "book@fr                   ... no changes\n",
      "book@he                   ... no changes\n",
      "book@hi                   ... no changes\n",
      "book@id                   ... no changes\n",
      "book@ja                   ... no changes\n",
      "book@ko                   ... no changes\n",
      "book@la                   ... no changes\n",
      "book@nl                   ... no changes\n",
      "book@pa                   ... no changes\n",
      "book@pt                   ... no changes\n",
      "book@ru                   ... no changes\n",
      "book@sw                   ... no changes\n",
      "book@syc                  ... no changes\n",
      "book@tr                   ... no changes\n",
      "book@ur                   ... no changes\n",
      "book@yo                   ... no changes\n",
      "book@zh                   ... no changes\n",
      "chapter                   ... no changes\n",
      "code                      ... no changes\n",
      "det                       ... no changes\n",
      "dist                      ... no changes\n",
      "dist_unit                 ... no changes\n",
      "distributional_parent     ... no changes\n",
      "domain                    ... no changes\n",
      "freq_lex                  ... no changes\n",
      "freq_occ                  ... no changes\n",
      "function                  ... no changes\n",
      "functional_parent         ... no changes\n",
      "g_cons                    ... no changes\n",
      "g_cons_utf8               ... no changes\n",
      "g_lex                     ... no changes\n",
      "g_lex_utf8                ... no changes\n",
      "g_nme                     ... no changes\n",
      "g_nme_utf8                ... no changes\n",
      "g_pfm                     ... no changes\n",
      "g_pfm_utf8                ... no changes\n",
      "g_prs                     ... no changes\n",
      "g_prs_utf8                ... no changes\n",
      "g_uvf                     ... no changes\n",
      "g_uvf_utf8                ... no changes\n",
      "g_vbe                     ... no changes\n",
      "g_vbe_utf8                ... no changes\n",
      "g_vbs                     ... no changes\n",
      "g_vbs_utf8                ... no changes\n",
      "g_voc_lex                 ... no changes\n",
      "g_voc_lex_utf8            ... no changes\n",
      "g_word                    ... no changes\n",
      "g_word_utf8               ... no changes\n",
      "gn                        ... no changes\n",
      "is_root                   ... no changes\n",
      "kind                      ... no changes\n",
      "label                     ... no changes\n",
      "language                  ... no changes\n",
      "lex                       ... no changes\n",
      "lex_utf8                  ... no changes\n",
      "ls                        ... no changes\n",
      "mother                    ... no changes\n",
      "mother_object_type        ... no changes\n",
      "nme                       ... no changes\n",
      "nu                        ... no changes\n",
      "number                    ... no changes\n",
      "oslots                    ... no changes\n",
      "otext                     ... no changes\n",
      "otype                     ... no changes\n",
      "pdp                       ... no changes\n",
      "pfm                       ... no changes\n",
      "prs                       ... no changes\n",
      "prs_gn                    ... no changes\n",
      "prs_nu                    ... no changes\n",
      "prs_ps                    ... no changes\n",
      "ps                        ... no changes\n",
      "qere                      ... no changes\n",
      "qere_utf8                 ... no changes\n",
      "rank_lex                  ... no changes\n",
      "rank_occ                  ... no changes\n",
      "rela                      ... no changes\n",
      "sp                        ... no changes\n",
      "st                        ... no changes\n",
      "tab                       ... no changes\n",
      "trailer                   ... no changes\n",
      "trailer_utf8              ... no changes\n",
      "txt                       ... no changes\n",
      "typ                       ... no changes\n",
      "uvf                       ... no changes\n",
      "vbe                       ... no changes\n",
      "vbs                       ... no changes\n",
      "verse                     ... no changes\n",
      "vs                        ... no changes\n",
      "vt                        ... no changes\n"
     ]
    }
   ],
   "source": [
    "checkDiffs(thisSave, thisDeliver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deliverDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compileTfData()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
